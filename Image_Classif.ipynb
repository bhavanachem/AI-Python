{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUGR8A20+hdyUwfl6qXNK3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhavanachem/AI-Python/blob/main/Image_Classif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oaVht_m9dKFJ"
      },
      "outputs": [],
      "source": [
        "!pip install pycocotools #web input\n",
        "!pip install ffmpeg\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "work_dir = \"/content/drive/MyDrive/SKILLIT Courses/AI Level 2/Final Project\"\n",
        "os.chdir(work_dir)\n",
        "\n",
        "from colab_utils import imshow, videoGrabber    #helps show images on colab screen\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import output   #to clear screen\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm      #status bar, training model status\n",
        "\n",
        "#define classes\n",
        "CLASS_NAME = ['Water Bottle', 'Chair', 'Bag']   #adr 0,1,2\n",
        "#define image capture function\n",
        "\n",
        "def capture_images(numImage=50, label = 'Null'):\n",
        "  vid = videoGrabber(showVideo=True, size = (60,40))  #videograbber imported\n",
        "  img = []\n",
        "  y=[]\n",
        "\n",
        "  for x in tqdm(range(numImage)):\n",
        "    new_image = np.array(vid(0))  #latest image saved in new_image\n",
        "    img.append(new_image)\n",
        "    if label!='Null':\n",
        "      y.append(label) #saving either rock,paper,scissor\n",
        "  img = np.array(img)\n",
        "  y = np.array(y)       #converting to numpy array (easier)\n",
        "  return img,y\n",
        "\n",
        "#now you have 100 images each for rock paper scissors\n",
        "#each pixel has val bet 0 and 255 depending upon its brightness\n",
        "#convert pixels bet 0 and 1 to match weights in neural network\n",
        "\n",
        "bottle_images, bottle_label = capture_images(numImage = 100, label=0)\n",
        "chair_images, chair_label = capture_images(numImage=100, label = 1)\n",
        "bag_images, bag_label = capture_images(numImage=100, label = 2)\n",
        "\n",
        "train_images = np.concatenate((bottle_images, chair_images, bag_images))\n",
        "train_images = train_images/255\n",
        "train_labels = np.concatenate((bottle_label, chair_label, bag_label))\n",
        "\n",
        "#300 images may not be enough for the model to learn from, so change features to make more images\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#creates a data generator obj that transforms images\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range = 40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "idx=0\n",
        "new_train_images = []\n",
        "new_train_labels = []\n",
        "for image in train_images:\n",
        "  img = image.reshape((1,)+image.shape) #make image 3d, 1x60x40 bc cant tranform without 3d\n",
        "  i=0\n",
        "  for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):\n",
        "    plt.figure(i)\n",
        "    #print(batch[0])\n",
        "    plot = plt.imshow(batch[0])\n",
        "    i+=1\n",
        "    new_train_images.append(batch[0])\n",
        "    new_train_labels.append(train_labels[idx])\n",
        "    if i > 10:\n",
        "      break\n",
        "  idx+=1\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "new_train_images=np.array(new_train_images)\n",
        "new_train_labels=np.array(new_train_labels)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# need to classify images into R,P,S\n",
        "#dense layer connected, received input from last\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation = 'relu'))\n",
        "model.add(layers.Dense(3))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#training the model\n",
        "\n",
        "model.compile(optimizer = 'adam', loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(new_train_images, new_train_labels,epochs=10)\n",
        "\n",
        "# Extracting accuracy values from the training output\n",
        "accuracies = [0.8576, 0.9767, 0.9906, 0.9876, 0.9958, 0.9985, 0.9985, 0.9973, 0.9994, 1.0000]\n",
        "\n",
        "# Calculating average accuracy\n",
        "average_accuracy = sum(accuracies) / len(accuracies)\n",
        "print(f\"Average accuracy: {average_accuracy}\")\n",
        "\n",
        "!pip install gTTS\n",
        "from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "\n",
        "\n",
        "# Function to convert text to speech\n",
        "def text_to_speech(text):\n",
        "    tts = gTTS(text=text, lang='en')\n",
        "    tts.save('output.mp3')\n",
        "    return 'output.mp3'\n",
        "\n",
        "#output calculation\n",
        "\n",
        "test_image, _ = capture_images(1)  #label not defined yet\n",
        "\n",
        "test_image = test_image/255 #pixel 0-1\n",
        "prediction = model.predict(test_image)\n",
        "plt.imshow(test_image[0])\n",
        "plt.title(CLASS_NAME[np.argmax(prediction[0])])\n",
        "\n",
        "#speak out\n",
        "\n",
        "# Text you want to convert to speech\n",
        "text = CLASS_NAME[np.argmax(prediction[0])]\n",
        "\n",
        "\n",
        "# Convert text to speech and save as an audio file\n",
        "audio_file = text_to_speech(text)\n",
        "\n",
        "\n",
        "# Play the audio file\n",
        "Audio(audio_file, autoplay=True)\n",
        "\n",
        "model.save('self-made_model.h5') #saving tensorflow = .h5"
      ]
    }
  ]
}